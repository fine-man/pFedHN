{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some magic so that the notebook will reload external python modules;\n",
    "# see https://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from tqdm import trange\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfedhn_pc.models import CNNHyperPC, CNNTargetPC, LocalLayer\n",
    "from pfedhn_pc.node import BaseNodesForLocal\n",
    "from utils import get_device, set_logger, set_seed, str2bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(nodes, num_nodes, hnet, net, criteria, device, split):\n",
    "    curr_results = evaluate(nodes, num_nodes, hnet, net, criteria, device, split=split)\n",
    "    total_correct = sum([val['correct'] for val in curr_results.values()])\n",
    "    total_samples = sum([val['total'] for val in curr_results.values()])\n",
    "    avg_loss = np.mean([val['loss'] for val in curr_results.values()])\n",
    "    avg_acc = total_correct / total_samples\n",
    "\n",
    "    all_acc = [val['correct'] / val['total'] for val in curr_results.values()]\n",
    "\n",
    "    return curr_results, avg_loss, avg_acc, all_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(nodes: BaseNodesForLocal, num_nodes, hnet, net, criteria, device, split='test'):\n",
    "    hnet.eval()\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for node_id in range(num_nodes):  # iterating over nodes\n",
    "\n",
    "        running_loss, running_correct, running_samples = 0., 0., 0.\n",
    "        if split == 'test':\n",
    "            curr_data = nodes.test_loaders[node_id]\n",
    "        elif split == 'val':\n",
    "            curr_data = nodes.val_loaders[node_id]\n",
    "        else:\n",
    "            curr_data = nodes.train_loaders[node_id]\n",
    "\n",
    "        weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
    "        net.load_state_dict(weights)\n",
    "\n",
    "        for batch_count, batch in enumerate(curr_data):\n",
    "            img, label = tuple(t.to(device) for t in batch)\n",
    "            net_out = net(img)\n",
    "            pred = nodes.local_layers[node_id](net_out)\n",
    "            running_loss += criteria(pred, label).item()\n",
    "            running_correct += pred.argmax(1).eq(label).sum().item()\n",
    "            running_samples += len(label)\n",
    "\n",
    "        results[node_id]['loss'] = running_loss / (batch_count + 1)\n",
    "        results[node_id]['correct'] = running_correct\n",
    "        results[node_id]['total'] = running_samples\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_name: str, data_path: str, classes_per_node: int, num_nodes: int,\n",
    "          steps: int, inner_steps: int, optim: str, lr: float, inner_lr: float,\n",
    "          embed_lr: float, wd: float, inner_wd: float, embed_dim: int, hyper_hid: int,\n",
    "          n_hidden: int, n_kernels: int, bs: int, device, eval_every: int, save_path: Path,\n",
    "          ) -> None:\n",
    "\n",
    "    ###############################\n",
    "    # init nodes, hnet, local net #\n",
    "    ###############################\n",
    "\n",
    "    nodes = BaseNodesForLocal(\n",
    "        data_name=data_name,\n",
    "        data_path=data_path,\n",
    "        n_nodes=num_nodes,\n",
    "        base_layer=LocalLayer,\n",
    "        layer_config={'n_input': 84, 'n_output': 10 if data_name == 'cifar10' else 100},\n",
    "        base_optimizer=torch.optim.SGD, optimizer_config=dict(lr=inner_lr, momentum=.9, weight_decay=inner_wd),\n",
    "        device=device,\n",
    "        batch_size=bs,\n",
    "        classes_per_node=classes_per_node,\n",
    "    )\n",
    "\n",
    "    embed_dim = embed_dim\n",
    "    if embed_dim == -1:\n",
    "        logging.info(\"auto embedding size\")\n",
    "        embed_dim = int(1 + num_nodes / 4)\n",
    "\n",
    "    hnet = CNNHyperPC(\n",
    "        num_nodes, embed_dim, hidden_dim=hyper_hid, n_hidden=n_hidden,\n",
    "        n_kernels=n_kernels\n",
    "    )\n",
    "    net = CNNTargetPC(n_kernels=n_kernels)\n",
    "\n",
    "    hnet = hnet.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    ##################\n",
    "    # init optimizer #\n",
    "    ##################\n",
    "    embed_lr = embed_lr if embed_lr is not None else lr\n",
    "    optimizers = {\n",
    "        'sgd': torch.optim.SGD(\n",
    "            [\n",
    "                {'params': [p for n, p in hnet.named_parameters() if 'embed' not in n]},\n",
    "                {'params': [p for n, p in hnet.named_parameters() if 'embed' in n], 'lr': embed_lr}\n",
    "            ], lr=lr, momentum=0.9, weight_decay=wd\n",
    "        ),\n",
    "        'adam': torch.optim.Adam(params=hnet.parameters(), lr=lr)\n",
    "    }\n",
    "    optimizer = optimizers[optim]\n",
    "    criteria = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ################\n",
    "    # init metrics #\n",
    "    ################\n",
    "    last_eval = -1\n",
    "    best_step = -1\n",
    "    best_acc = -1\n",
    "    test_best_based_on_step, test_best_min_based_on_step = -1, -1\n",
    "    test_best_max_based_on_step, test_best_std_based_on_step = -1, -1\n",
    "    step_iter = trange(steps)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for step in step_iter:\n",
    "        hnet.train()\n",
    "\n",
    "        # select client at random\n",
    "        node_id = random.choice(range(num_nodes))\n",
    "\n",
    "        # produce & load local network weights\n",
    "        weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
    "        net.load_state_dict(weights)\n",
    "\n",
    "        # init inner optimizer\n",
    "        inner_optim = torch.optim.SGD(\n",
    "            net.parameters(), lr=inner_lr, momentum=.9, weight_decay=inner_wd\n",
    "        )\n",
    "\n",
    "        # storing theta_i for later calculating delta theta\n",
    "        inner_state = OrderedDict({k: tensor.data for k, tensor in weights.items()})\n",
    "\n",
    "        # NOTE: evaluation on sent model\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            batch = next(iter(nodes.test_loaders[node_id]))\n",
    "            img, label = tuple(t.to(device) for t in batch)\n",
    "            net_out = net(img)\n",
    "            pred = nodes.local_layers[node_id](net_out)\n",
    "            prvs_loss = criteria(pred, label)\n",
    "            prvs_acc = pred.argmax(1).eq(label).sum().item() / len(label)\n",
    "            net.train()\n",
    "\n",
    "        # inner updates -> obtaining theta_tilda\n",
    "        for i in range(inner_steps):\n",
    "            net.train()\n",
    "            inner_optim.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            nodes.local_optimizers[node_id].zero_grad()\n",
    "\n",
    "            batch = next(iter(nodes.train_loaders[node_id]))\n",
    "            img, label = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            net_out = net(img)\n",
    "            pred = nodes.local_layers[node_id](net_out)\n",
    "\n",
    "            loss = criteria(pred, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 50)\n",
    "            inner_optim.step()\n",
    "            nodes.local_optimizers[node_id].step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        final_state = net.state_dict()\n",
    "\n",
    "        # Calculating MSE Loss for the predicted HyperNetwork weights\n",
    "        hn_loss = 0.0\n",
    "        for key in weights.keys():\n",
    "            weight_loss = nn.MSELoss()(inner_state[key], final_state[key])\n",
    "            hn_loss += weight_loss\n",
    "\n",
    "        # calculating delta theta\n",
    "        delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
    "\n",
    "        # calculating phi gradient\n",
    "        hnet_grads = torch.autograd.grad(\n",
    "            list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
    "        )\n",
    "\n",
    "        # update hnet weights\n",
    "        for p, g in zip(hnet.parameters(), hnet_grads):\n",
    "            p.grad = g\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(hnet.parameters(), 50)\n",
    "        optimizer.step()\n",
    "\n",
    "        step_iter.set_description(\n",
    "            f\"Step: {step+1}, Node ID: {node_id}, Loss: {prvs_loss:.4f},  Acc: {prvs_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if step % eval_every == 0:\n",
    "            last_eval = step\n",
    "            step_results, avg_loss, avg_acc, all_acc = eval_model(\n",
    "                nodes, num_nodes, hnet, net, criteria, device, split=\"test\"\n",
    "            )\n",
    "            logging.info(f\"\\nStep: {step+1}, AVG Loss: {avg_loss:.4f},  AVG Acc: {avg_acc:.4f}, HN Loss: {hn_loss}\")\n",
    "\n",
    "            results['test_avg_loss'].append(avg_loss)\n",
    "            results['test_avg_acc'].append(avg_acc)\n",
    "\n",
    "            _, val_avg_loss, val_avg_acc, _ = eval_model(nodes, num_nodes, hnet, net, criteria, device, split=\"val\")\n",
    "            if best_acc < val_avg_acc:\n",
    "                best_acc = val_avg_acc\n",
    "                best_step = step\n",
    "                test_best_based_on_step = avg_acc\n",
    "                test_best_min_based_on_step = np.min(all_acc)\n",
    "                test_best_max_based_on_step = np.max(all_acc)\n",
    "                test_best_std_based_on_step = np.std(all_acc)\n",
    "\n",
    "            results['val_avg_loss'].append(val_avg_loss)\n",
    "            results['val_avg_acc'].append(val_avg_acc)\n",
    "            results['best_step'].append(best_step)\n",
    "            results['best_val_acc'].append(best_acc)\n",
    "            results['best_test_acc_based_on_val_beststep'].append(test_best_based_on_step)\n",
    "            results['test_best_min_based_on_step'].append(test_best_min_based_on_step)\n",
    "            results['test_best_max_based_on_step'].append(test_best_max_based_on_step)\n",
    "            results['test_best_std_based_on_step'].append(test_best_std_based_on_step)\n",
    "\n",
    "        \n",
    "        # Wandb logging\n",
    "        wandb_dict = defaultdict(int)\n",
    "        wandb_dict[\"step\"] = step\n",
    "        wandb_dict[\"hn_loss\"] = hn_loss.detach().item()\n",
    "        for key, value_list in results.items():\n",
    "            wandb_dict[key] = value_list[-1]\n",
    "        # wandb.log(wandb_dict)\n",
    "\n",
    "    if step != last_eval:\n",
    "        _, val_avg_loss, val_avg_acc, _ = eval_model(nodes, num_nodes, hnet, net, criteria, device, split=\"val\")\n",
    "        step_results, avg_loss, avg_acc, all_acc = eval_model(nodes, num_nodes, hnet, net, criteria, device, split=\"test\")\n",
    "        logging.info(f\"\\nStep: {step + 1}, AVG Loss: {avg_loss:.4f},  AVG Acc: {avg_acc:.4f}\")\n",
    "\n",
    "        results['test_avg_loss'].append(avg_loss)\n",
    "        results['test_avg_acc'].append(avg_acc)\n",
    "\n",
    "        if best_acc < val_avg_acc:\n",
    "            best_acc = val_avg_acc\n",
    "            best_step = step\n",
    "            test_best_based_on_step = avg_acc\n",
    "            test_best_min_based_on_step = np.min(all_acc)\n",
    "            test_best_max_based_on_step = np.max(all_acc)\n",
    "            test_best_std_based_on_step = np.std(all_acc)\n",
    "\n",
    "        results['val_avg_loss'].append(val_avg_loss)\n",
    "        results['val_avg_acc'].append(val_avg_acc)\n",
    "        results['best_step'].append(best_step)\n",
    "        results['best_val_acc'].append(best_acc)\n",
    "        results['best_test_acc_based_on_val_beststep'].append(test_best_based_on_step)\n",
    "        results['test_best_min_based_on_step'].append(test_best_min_based_on_step)\n",
    "        results['test_best_max_based_on_step'].append(test_best_max_based_on_step)\n",
    "        results['test_best_std_based_on_step'].append(test_best_std_based_on_step)\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(str(save_path / \"results.json\"), \"w\") as file:\n",
    "        json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    diff = (x - y).flatten()\n",
    "    idx = torch.argmax(diff.abs())\n",
    "    sign = 1 if diff[idx] >= 0 else -1\n",
    "    return sign * torch.max(torch.abs(x - y)/(torch.maximum(torch.abs(x), torch.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6000)\n"
     ]
    }
   ],
   "source": [
    "a = 5 * torch.ones(5)\n",
    "b = 2 * torch.ones(5)\n",
    "print(rel_error(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mse(\n",
    "    data_name: str,\n",
    "    data_path: str,\n",
    "    classes_per_node: int,\n",
    "    num_nodes: int,\n",
    "    steps: int,\n",
    "    inner_steps: int,\n",
    "    optim: str,\n",
    "    lr: float,\n",
    "    inner_lr: float,\n",
    "    embed_lr: float,\n",
    "    wd: float,\n",
    "    inner_wd: float,\n",
    "    embed_dim: int,\n",
    "    hyper_hid: int,\n",
    "    n_hidden: int,\n",
    "    n_kernels: int,\n",
    "    bs: int,\n",
    "    device,\n",
    "    eval_every: int,\n",
    "    save_path: Path,\n",
    ") -> None:\n",
    "    ###############################\n",
    "    # init nodes, hnet, local net #\n",
    "    ###############################\n",
    "\n",
    "    nodes = BaseNodesForLocal(\n",
    "        data_name=data_name,\n",
    "        data_path=data_path,\n",
    "        n_nodes=num_nodes,\n",
    "        base_layer=LocalLayer,\n",
    "        layer_config={\"n_input\": 84, \"n_output\": 10 if data_name == \"cifar10\" else 100},\n",
    "        base_optimizer=torch.optim.SGD,\n",
    "        optimizer_config=dict(lr=inner_lr, momentum=0.9, weight_decay=inner_wd),\n",
    "        device=device,\n",
    "        batch_size=bs,\n",
    "        classes_per_node=classes_per_node,\n",
    "    )\n",
    "\n",
    "    embed_dim = embed_dim\n",
    "    if embed_dim == -1:\n",
    "        logging.info(\"auto embedding size\")\n",
    "        embed_dim = int(1 + num_nodes / 4)\n",
    "\n",
    "    hnet = CNNHyperPC(\n",
    "        num_nodes,\n",
    "        embed_dim,\n",
    "        hidden_dim=hyper_hid,\n",
    "        n_hidden=n_hidden,\n",
    "        n_kernels=n_kernels,\n",
    "    )\n",
    "    net = CNNTargetPC(n_kernels=n_kernels)\n",
    "\n",
    "    hnet = hnet.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    ##################\n",
    "    # init optimizer #\n",
    "    ##################\n",
    "    embed_lr = embed_lr if embed_lr is not None else lr\n",
    "    optimizers = {\n",
    "        \"sgd\": torch.optim.SGD(\n",
    "            [\n",
    "                {\"params\": [p for n, p in hnet.named_parameters() if \"embed\" not in n]},\n",
    "                {\n",
    "                    \"params\": [p for n, p in hnet.named_parameters() if \"embed\" in n],\n",
    "                    \"lr\": embed_lr,\n",
    "                },\n",
    "            ],\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=wd,\n",
    "        ),\n",
    "        \"adam\": torch.optim.Adam(params=hnet.parameters(), lr=lr),\n",
    "    }\n",
    "    optimizer = optimizers[optim]\n",
    "    criteria = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ################\n",
    "    # init metrics #\n",
    "    ################\n",
    "    last_eval = -1\n",
    "    best_step = -1\n",
    "    best_acc = -1\n",
    "    test_best_based_on_step, test_best_min_based_on_step = -1, -1\n",
    "    test_best_max_based_on_step, test_best_std_based_on_step = -1, -1\n",
    "    # steps = 1\n",
    "    step_iter = trange(steps)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for step in step_iter:\n",
    "        hnet.train()\n",
    "\n",
    "        # select client at random\n",
    "        node_id = random.choice(range(num_nodes))\n",
    "\n",
    "        # produce & load local network weights\n",
    "        weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
    "        net.load_state_dict(weights)\n",
    "\n",
    "        # init inner optimizer\n",
    "        inner_optim = torch.optim.SGD(\n",
    "            net.parameters(), lr=inner_lr, momentum=0.9, weight_decay=inner_wd\n",
    "        )\n",
    "\n",
    "        # storing theta_i for later calculating delta theta\n",
    "        inner_state = OrderedDict({k: tensor for k, tensor in weights.items()})\n",
    "\n",
    "        # NOTE: evaluation on sent model\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            batch = next(iter(nodes.test_loaders[node_id]))\n",
    "            img, label = tuple(t.to(device) for t in batch)\n",
    "            net_out = net(img)\n",
    "            pred = nodes.local_layers[node_id](net_out)\n",
    "            prvs_loss = criteria(pred, label)\n",
    "            prvs_acc = pred.argmax(1).eq(label).sum().item() / len(label)\n",
    "            net.train()\n",
    "\n",
    "        # inner updates -> obtaining theta_tilda\n",
    "        for i in range(inner_steps):\n",
    "            net.train()\n",
    "            inner_optim.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            nodes.local_optimizers[node_id].zero_grad()\n",
    "\n",
    "            batch = next(iter(nodes.train_loaders[node_id]))\n",
    "            img, label = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            net_out = net(img)\n",
    "            pred = nodes.local_layers[node_id](net_out)\n",
    "\n",
    "            loss = criteria(pred, label)\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 50)\n",
    "            inner_optim.step()\n",
    "            nodes.local_optimizers[node_id].step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        final_state = net.state_dict()\n",
    "\n",
    "        # Calculating MSE Loss for the predicted HyperNetwork weights\n",
    "        hn_loss = 0.0\n",
    "        for key in weights.keys():\n",
    "            weight_loss = nn.MSELoss(reduction='sum')(inner_state[key], final_state[key])\n",
    "            hn_loss += weight_loss\n",
    "\n",
    "        # hn_loss /= 2\n",
    "        # print(f\"Dividing by 2\")\n",
    "        hn_loss.backward()\n",
    "\n",
    "        grad_clones = defaultdict(str)\n",
    "        # print(\"\\nHello\")\n",
    "        for name, param in hnet.named_parameters():\n",
    "            # print(f\"{name}: {param.grad.shape}\")\n",
    "            grad_clones[name] = param.clone()\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        # # calculating delta theta\n",
    "        # delta_theta = OrderedDict(\n",
    "        #     {k: inner_state[k] - final_state[k] for k in weights.keys()}\n",
    "        # )\n",
    "\n",
    "        # # calculating phi gradient\n",
    "        # hnet_grads = torch.autograd.grad(\n",
    "        #     list(weights.values()),\n",
    "        #     hnet.parameters(),\n",
    "        #     grad_outputs=list(delta_theta.values()),\n",
    "        # )\n",
    "\n",
    "        # # update hnet weights\n",
    "        # for p, g in zip(hnet.parameters(), hnet_grads):\n",
    "        #     p.grad = g\n",
    "\n",
    "       ########## \n",
    "       # Printing the difference\n",
    "       ########## \n",
    "        # print(\"\\nDifferences:\")\n",
    "        # for name, params in hnet.named_parameters():\n",
    "        #     new_grad = params.grad\n",
    "        #     old_grad = grad_clones[name]\n",
    "        #     err = rel_error(new_grad, old_grad)\n",
    "        #     print(f\"{name}: {err}\")\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(hnet.parameters(), 50)\n",
    "        optimizer.step()\n",
    "\n",
    "        step_iter.set_description(\n",
    "            f\"Step: {step+1}, Node ID: {node_id}, Loss: {prvs_loss:.4f},  Acc: {prvs_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if step % eval_every == 0:\n",
    "            last_eval = step\n",
    "            step_results, avg_loss, avg_acc, all_acc = eval_model(\n",
    "                nodes, num_nodes, hnet, net, criteria, device, split=\"test\"\n",
    "            )\n",
    "\n",
    "            logging.info(f\"\\n\\nStep: {step+1}, AVG Loss: {avg_loss:.4f},  AVG Acc: {avg_acc:.4f} | HN Loss: {hn_loss}\")\n",
    "\n",
    "            results[\"test_avg_loss\"].append(avg_loss)\n",
    "            results[\"test_avg_acc\"].append(avg_acc)\n",
    "\n",
    "            _, val_avg_loss, val_avg_acc, _ = eval_model(\n",
    "                nodes, num_nodes, hnet, net, criteria, device, split=\"val\"\n",
    "            )\n",
    "            if best_acc < val_avg_acc:\n",
    "                best_acc = val_avg_acc\n",
    "                best_step = step\n",
    "                test_best_based_on_step = avg_acc\n",
    "                test_best_min_based_on_step = np.min(all_acc)\n",
    "                test_best_max_based_on_step = np.max(all_acc)\n",
    "                test_best_std_based_on_step = np.std(all_acc)\n",
    "\n",
    "            results[\"val_avg_loss\"].append(val_avg_loss)\n",
    "            results[\"val_avg_acc\"].append(val_avg_acc)\n",
    "            results[\"best_step\"].append(best_step)\n",
    "            results[\"best_val_acc\"].append(best_acc)\n",
    "            results[\"best_test_acc_based_on_val_beststep\"].append(\n",
    "                test_best_based_on_step\n",
    "            )\n",
    "            results[\"test_best_min_based_on_step\"].append(test_best_min_based_on_step)\n",
    "            results[\"test_best_max_based_on_step\"].append(test_best_max_based_on_step)\n",
    "            results[\"test_best_std_based_on_step\"].append(test_best_std_based_on_step)\n",
    "\n",
    "            # weights_dict = defaultdict(int)\n",
    "            # for name, param in hnet.named_parameters():\n",
    "            #     weights_dict[name] = param.detach().norm().item()\n",
    "            #     print(f\"{name}: {weights_dict[name]}\")\n",
    "        \n",
    "        # Wandb logging\n",
    "        wandb_dict = defaultdict(int)\n",
    "        wandb_dict[\"step\"] = step\n",
    "        wandb_dict[\"hn_loss\"] = hn_loss.detach().item()\n",
    "        for key, value_list in results.items():\n",
    "            wandb_dict[key] = value_list[-1]\n",
    "        # wandb.log(wandb_dict)\n",
    "\n",
    "    if step != last_eval:\n",
    "        _, val_avg_loss, val_avg_acc, _ = eval_model(\n",
    "            nodes, num_nodes, hnet, net, criteria, device, split=\"val\"\n",
    "        )\n",
    "        step_results, avg_loss, avg_acc, all_acc = eval_model(\n",
    "            nodes, num_nodes, hnet, net, criteria, device, split=\"test\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"\\nStep: {step + 1}, AVG Loss: {avg_loss:.4f},  AVG Acc: {avg_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"test_avg_loss\"].append(avg_loss)\n",
    "        results[\"test_avg_acc\"].append(avg_acc)\n",
    "\n",
    "        if best_acc < val_avg_acc:\n",
    "            best_acc = val_avg_acc\n",
    "            best_step = step\n",
    "            test_best_based_on_step = avg_acc\n",
    "            test_best_min_based_on_step = np.min(all_acc)\n",
    "            test_best_max_based_on_step = np.max(all_acc)\n",
    "            test_best_std_based_on_step = np.std(all_acc)\n",
    "\n",
    "        results[\"val_avg_loss\"].append(val_avg_loss)\n",
    "        results[\"val_avg_acc\"].append(val_avg_acc)\n",
    "        results[\"best_step\"].append(best_step)\n",
    "        results[\"best_val_acc\"].append(best_acc)\n",
    "        results[\"best_test_acc_based_on_val_beststep\"].append(test_best_based_on_step)\n",
    "        results[\"test_best_min_based_on_step\"].append(test_best_min_based_on_step)\n",
    "        results[\"test_best_max_based_on_step\"].append(test_best_max_based_on_step)\n",
    "        results[\"test_best_std_based_on_step\"].append(test_best_std_based_on_step)\n",
    "\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(str(save_path / \"results.json\"), \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Federated Hypernetwork with local layers experiment\"\n",
    ")\n",
    "\n",
    "#############################\n",
    "#       Dataset Args        #\n",
    "#############################\n",
    "parser.add_argument(\n",
    "    \"--data-name\", type=str, default=\"cifar10\", choices=['cifar10', 'cifar100'], help=\"data name\"\n",
    ")\n",
    "parser.add_argument(\"--data-path\", type=str, default='data', help='data path')\n",
    "parser.add_argument(\"--num-nodes\", type=int, default=50)\n",
    "\n",
    "##################################\n",
    "#       Optimization args        #\n",
    "##################################\n",
    "parser.add_argument(\"--num-steps\", type=int, default=5000)\n",
    "parser.add_argument(\"--batch-size\", type=int, default=64)\n",
    "parser.add_argument(\"--inner-steps\", type=int, default=50, help=\"number of inner steps\")\n",
    "parser.add_argument(\"--optim\", type=str, default='sgd', choices=['adam', 'sgd'], help=\"learning rate\")\n",
    "\n",
    "################################\n",
    "#       Model Prop args        #\n",
    "################################\n",
    "parser.add_argument(\"--n-hidden\", type=int, default=3, help=\"num. hidden layers\")\n",
    "parser.add_argument(\"--inner-lr\", type=float, default=5e-3, help=\"learning rate for inner optimizer\")\n",
    "parser.add_argument(\"--lr\", type=float, default=5e-2, help=\"learning rate\")\n",
    "parser.add_argument(\"--wd\", type=float, default=1e-3, help=\"weight decay\")\n",
    "parser.add_argument(\"--inner-wd\", type=float, default=5e-5, help=\"inner weight decay\")\n",
    "parser.add_argument(\"--embed-dim\", type=int, default=-1, help=\"embedding dim\")\n",
    "parser.add_argument(\"--embed-lr\", type=float, default=None, help=\"embedding learning rate\")\n",
    "parser.add_argument(\"--hyper-hid\", type=int, default=100, help=\"hypernet hidden dim\")\n",
    "parser.add_argument(\"--spec-norm\", type=str2bool, default=False, help=\"hypernet hidden dim\")\n",
    "parser.add_argument(\"--nkernels\", type=int, default=16, help=\"number of kernels for cnn model\")\n",
    "\n",
    "#############################\n",
    "#       General args        #\n",
    "#############################\n",
    "parser.add_argument(\"--gpu\", type=int, default=0, help=\"gpu device ID\")\n",
    "parser.add_argument(\"--eval-every\", type=int, default=30, help=\"eval every X selected epochs\")\n",
    "parser.add_argument(\"--save-path\", type=str, default=\"pfedhn_pc_cifar_res\", help=\"dir path for output file\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"seed value\")\n",
    "\n",
    "args = parser.parse_args({})\n",
    "assert args.gpu <= torch.cuda.device_count(), f\"--gpu flag should be in range [0,{torch.cuda.device_count() - 1}]\"\n",
    "args.gpu = 1\n",
    "\n",
    "set_logger()\n",
    "set_seed(args.seed)\n",
    "\n",
    "device = get_device(gpus=args.gpu)\n",
    "\n",
    "if args.data_name == 'cifar10':\n",
    "    args.classes_per_node = 2\n",
    "else:\n",
    "    args.classes_per_node = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\n\u001b[1;32m      2\u001b[0m     data_name\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_name,\n\u001b[1;32m      3\u001b[0m     data_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m      4\u001b[0m     classes_per_node\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mclasses_per_node,\n\u001b[1;32m      5\u001b[0m     num_nodes\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_nodes,\n\u001b[1;32m      6\u001b[0m     steps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_steps,\n\u001b[1;32m      7\u001b[0m     inner_steps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minner_steps,\n\u001b[1;32m      8\u001b[0m     optim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39moptim,\n\u001b[1;32m      9\u001b[0m     lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr,\n\u001b[1;32m     10\u001b[0m     inner_lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minner_lr,\n\u001b[1;32m     11\u001b[0m     embed_lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39membed_lr,\n\u001b[1;32m     12\u001b[0m     wd\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwd,\n\u001b[1;32m     13\u001b[0m     inner_wd\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minner_wd,\n\u001b[1;32m     14\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m     15\u001b[0m     hyper_hid\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhyper_hid,\n\u001b[1;32m     16\u001b[0m     n_hidden\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_hidden,\n\u001b[1;32m     17\u001b[0m     n_kernels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnkernels,\n\u001b[1;32m     18\u001b[0m     bs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     19\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     20\u001b[0m     eval_every\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meval_every,\n\u001b[1;32m     21\u001b[0m     save_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msave_path,\n\u001b[1;32m     22\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_name, data_path, classes_per_node, num_nodes, steps, inner_steps, optim, lr, inner_lr, embed_lr, wd, inner_wd, embed_dim, hyper_hid, n_hidden, n_kernels, bs, device, eval_every, save_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(data_name: \u001b[38;5;28mstr\u001b[39m, data_path: \u001b[38;5;28mstr\u001b[39m, classes_per_node: \u001b[38;5;28mint\u001b[39m, num_nodes: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      2\u001b[0m           steps: \u001b[38;5;28mint\u001b[39m, inner_steps: \u001b[38;5;28mint\u001b[39m, optim: \u001b[38;5;28mstr\u001b[39m, lr: \u001b[38;5;28mfloat\u001b[39m, inner_lr: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m      3\u001b[0m           embed_lr: \u001b[38;5;28mfloat\u001b[39m, wd: \u001b[38;5;28mfloat\u001b[39m, inner_wd: \u001b[38;5;28mfloat\u001b[39m, embed_dim: \u001b[38;5;28mint\u001b[39m, hyper_hid: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# init nodes, hnet, local net #\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m BaseNodesForLocal(\n\u001b[1;32m     12\u001b[0m         data_name\u001b[38;5;241m=\u001b[39mdata_name,\n\u001b[1;32m     13\u001b[0m         data_path\u001b[38;5;241m=\u001b[39mdata_path,\n\u001b[1;32m     14\u001b[0m         n_nodes\u001b[38;5;241m=\u001b[39mnum_nodes,\n\u001b[1;32m     15\u001b[0m         base_layer\u001b[38;5;241m=\u001b[39mLocalLayer,\n\u001b[1;32m     16\u001b[0m         layer_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_input\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m84\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_output\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m100\u001b[39m},\n\u001b[1;32m     17\u001b[0m         base_optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD, optimizer_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39minner_lr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39minner_wd),\n\u001b[1;32m     18\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     19\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbs,\n\u001b[1;32m     20\u001b[0m         classes_per_node\u001b[38;5;241m=\u001b[39mclasses_per_node,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     embed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/pFedHN/experiments/pfedhn_pc/node.py:25\u001b[0m, in \u001b[0;36mBaseNodesForLocal.__init__\u001b[0;34m(self, data_name, data_path, n_nodes, base_layer, layer_config, base_optimizer, optimizer_config, device, batch_size, classes_per_node)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_per_node \u001b[38;5;241m=\u001b[39m classes_per_node\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_layers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     base_layer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer_config)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_optimizers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     base_optimizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_layers[i]\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptimizer_config) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loaders \u001b[38;5;241m=\u001b[39m gen_random_loaders(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_name,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_per_node\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/pFedHN/experiments/pfedhn_pc/node.py:26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_per_node \u001b[38;5;241m=\u001b[39m classes_per_node\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_layers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 26\u001b[0m     base_layer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayer_config)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_optimizers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     base_optimizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_layers[i]\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptimizer_config) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loaders \u001b[38;5;241m=\u001b[39m gen_random_loaders(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_name,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_per_node\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    data_name=args.data_name,\n",
    "    data_path=args.data_path,\n",
    "    classes_per_node=args.classes_per_node,\n",
    "    num_nodes=args.num_nodes,\n",
    "    steps=args.num_steps,\n",
    "    inner_steps=args.inner_steps,\n",
    "    optim=args.optim,\n",
    "    lr=args.lr,\n",
    "    inner_lr=args.inner_lr,\n",
    "    embed_lr=args.embed_lr,\n",
    "    wd=args.wd,\n",
    "    inner_wd=args.inner_wd,\n",
    "    embed_dim=args.embed_dim,\n",
    "    hyper_hid=args.hyper_hid,\n",
    "    n_hidden=args.n_hidden,\n",
    "    n_kernels=args.nkernels,\n",
    "    bs=args.batch_size,\n",
    "    device=device,\n",
    "    eval_every=args.eval_every,\n",
    "    save_path=args.save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 18:07:34,898 - root - INFO - auto embedding size\n",
      "Step: 1, Node ID: 26, Loss: 2.5047,  Acc: 0.0156:   0%|          | 0/5000 [00:01<?, ?it/s]2024-01-27 18:07:39,940 - root - INFO - \n",
      "\n",
      "Step: 1, AVG Loss: 2.3861,  AVG Acc: 0.0944 | HN Loss: 1.178234577178955\n",
      "Step: 31, Node ID: 22, Loss: 2.5234,  Acc: 0.0000:   1%|          | 30/5000 [00:53<2:02:16,  1.48s/it]2024-01-27 18:08:31,350 - root - INFO - \n",
      "\n",
      "Step: 31, AVG Loss: 1.5649,  AVG Acc: 0.3725 | HN Loss: 0.5831138491630554\n",
      "Step: 61, Node ID: 46, Loss: 4.2321,  Acc: 0.0000:   1%|          | 60/5000 [01:43<1:58:51,  1.44s/it]2024-01-27 18:09:22,144 - root - INFO - \n",
      "\n",
      "Step: 61, AVG Loss: 1.2509,  AVG Acc: 0.5719 | HN Loss: 0.8777075409889221\n",
      "Step: 62, Node ID: 40, Loss: 0.5787,  Acc: 0.7500:   1%|          | 62/5000 [01:53<2:30:08,  1.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_mse(\n\u001b[1;32m      2\u001b[0m     data_name\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_name,\n\u001b[1;32m      3\u001b[0m     data_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m      4\u001b[0m     classes_per_node\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mclasses_per_node,\n\u001b[1;32m      5\u001b[0m     num_nodes\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_nodes,\n\u001b[1;32m      6\u001b[0m     steps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_steps,\n\u001b[1;32m      7\u001b[0m     inner_steps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minner_steps,\n\u001b[1;32m      8\u001b[0m     optim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39moptim,\n\u001b[1;32m      9\u001b[0m     lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr,\n\u001b[1;32m     10\u001b[0m     inner_lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minner_lr,\n\u001b[1;32m     11\u001b[0m     embed_lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39membed_lr,\n\u001b[1;32m     12\u001b[0m     wd\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwd,\n\u001b[1;32m     13\u001b[0m     inner_wd\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minner_wd,\n\u001b[1;32m     14\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m     15\u001b[0m     hyper_hid\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhyper_hid,\n\u001b[1;32m     16\u001b[0m     n_hidden\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_hidden,\n\u001b[1;32m     17\u001b[0m     n_kernels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnkernels,\n\u001b[1;32m     18\u001b[0m     bs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     19\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     20\u001b[0m     eval_every\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meval_every,\n\u001b[1;32m     21\u001b[0m     save_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msave_path,\n\u001b[1;32m     22\u001b[0m )\n",
      "Cell \u001b[0;32mIn[21], line 134\u001b[0m, in \u001b[0;36mtrain_mse\u001b[0;34m(data_name, data_path, classes_per_node, num_nodes, steps, inner_steps, optim, lr, inner_lr, embed_lr, wd, inner_wd, embed_dim, hyper_hid, n_hidden, n_kernels, bs, device, eval_every, save_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m pred \u001b[38;5;241m=\u001b[39m nodes\u001b[38;5;241m.\u001b[39mlocal_layers[node_id](net_out)\n\u001b[1;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteria(pred, label)\n\u001b[0;32m--> 134\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    137\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    138\u001b[0m inner_optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lip-reading/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_mse(\n",
    "    data_name=args.data_name,\n",
    "    data_path=args.data_path,\n",
    "    classes_per_node=args.classes_per_node,\n",
    "    num_nodes=args.num_nodes,\n",
    "    steps=args.num_steps,\n",
    "    inner_steps=args.inner_steps,\n",
    "    optim=args.optim,\n",
    "    lr=args.lr,\n",
    "    inner_lr=args.inner_lr,\n",
    "    embed_lr=args.embed_lr,\n",
    "    wd=args.wd,\n",
    "    inner_wd=args.inner_wd,\n",
    "    embed_dim=args.embed_dim,\n",
    "    hyper_hid=args.hyper_hid,\n",
    "    n_hidden=args.n_hidden,\n",
    "    n_kernels=args.nkernels,\n",
    "    bs=args.batch_size,\n",
    "    device=device,\n",
    "    eval_every=args.eval_every,\n",
    "    save_path=args.save_path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lip-reading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
